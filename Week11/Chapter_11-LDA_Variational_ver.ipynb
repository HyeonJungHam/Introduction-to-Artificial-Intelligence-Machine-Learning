{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA with Variational Inference\n",
    "###  <div align=center> Moon Il-chul(icmoon@kaist.ac.kr);<br/>Na Byeong-hu(wp03052@kaist.ac.kr); Kim Kyu-Seok(kimkyu80@kaist.ac.kr); Bae Hee-Sun (cat2507@kaist.ac.kr) </div>\n",
    "본 코드는 Variational Inference를 적용한 LDA를 20NewsGroups 데이터셋을 데이터로 이용하여 구현한 예시입니다. 본 코드는 기존의 LDA 모델에 Variational Inference를 적용한 과정 및 결과를 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA(Latent Dirichlet Allocation)\n",
    "LDA는 text data에 대해 확률 분포를 기준으로 이용하여 군집을 형성하는 soft clustering 방식의 모델입니다. \n",
    "LDA 문서 생성과정을 간단히 정리해보겠습니다.\n",
    "\n",
    "![screen](LDA.jpg)\n",
    "우선 변수\n",
    "\n",
    "1) $\\phi_{k}$: k번째 토픽에 해당하는 벡터.\n",
    "\n",
    "2) $\\theta_{d}$: d번째 문서가 가진 토픽 비중을 나타내는 벡터\n",
    "\n",
    "3) $z_{d,n}$: d번째 문서 n번째 단어가 어떤 토픽에 해당하는지 할당\n",
    "\n",
    "4)$w_{d,n}$: d번째 문서에 n번째 단어가 몇 번 나타나는가?\n",
    "\n",
    "를 정의하겠습니다. $\\phi_{k}$와 $\\theta_{d}$가 dirichlet distribution을 따른다는 가정을 하기 때문에 dirichlet이 이름에 들어갑니다.\n",
    "\n",
    "여기서 우리가 관찰할 수 있는 변수는 $w_{d,n}$ 뿐인데, 이것을 이용하여 잠재변수를 역으로 추정할 것입니다. 즉, LDA의 inference는 실제 관찰가능한 문서 내 단어를 가지고 우리가 알고 싶은 토픽의 단어분포, 문서의 토픽분포를 추정하는 과정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELBO of LDA with VI\n",
    "Variational Inference를 적용시킨 LDA에서의 ELBO는 다음과 같습니다.\n",
    "\n",
    "$L(\\gamma, \\phi|\\alpha, \\beta)$ \n",
    "\n",
    "$= E_q(logP(\\theta|\\alpha)) + E_q(logP(z|\\theta)) + E_q(logP(w|z,\\beta)) + H(q)$ \n",
    "\n",
    "$= \\sum_{d=1}^{M}\\sum_{i=1}^{k}(\\alpha_i-1)(-\\psi(\\sum_{i=1}^{K}\\gamma_{d,i})+\\psi(\\gamma_{d,i})) + log\\Gamma(\\sum_{i=1}^{K}\\alpha_i)-\\sum_{i=1}^{K}log\\Gamma(\\alpha_i)$\n",
    "\n",
    "$+ \\sum_{d=1}^{M}\\sum_{n=1}^{N_d}\\sum_{i=1}^{K}\\phi_{d,n,i}(-\\psi(\\sum_{i=1}^{K}\\gamma_{d,i}) + \\psi(\\gamma_{d,i}))$\n",
    "\n",
    "$+ \\sum_{d=1}^{M}\\sum_{n=1}^{N_d}\\sum_{i=1}^{K}\\phi_{d,n,i}log\\beta_{i,w_{d,n}}$\n",
    "\n",
    "$- \\sum_{d=1}^{M}\\sum_{i=1}^{K}(\\gamma_{d,i}-1)(-\\psi(\\sum_{i=1}^{K}\\gamma_{d,i}) + \\psi(\\gamma_{d,i})) - log\\Gamma(\\sum_{i=1}^{K}\\gamma_{d,i}) + \\sum_{i=1}^{K}log\\Gamma(\\gamma_{d,i})$\n",
    "\n",
    "$- \\sum_{d=1}^{M}\\sum_{n=1}^{N_d}\\sum_{i=1}^{K}\\phi_{d,n,i}log\\phi_{d,n,i}$\n",
    "\n",
    "위의 ELBO를 maximize하는 방향으로 variational parameter인 $φ,γ$ 와 model parameter인 $α,β$ 를 inference 할 것 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Learning\n",
    "Variational parameter $φ$ 과 $γ$ 를 learning하는 과정은 다음과 같습니다.\n",
    "\n",
    "$$ \\phi_{d,n,i} ∝ \\beta_{i,w_{d,n}}exp(\\psi(\\gamma{_d,_i})) $$\n",
    "\n",
    "$$ \\gamma_{d,i} = \\alpha_i + \\sum_{n=1}^{N_d}\\phi_{d,n,i} $$\n",
    "\n",
    "\n",
    "Model Parameter $α$ 와 $β$ 를 learning하는 과정은 다음과 같습니다.\n",
    "\n",
    "$$ \\alpha_{n+1} = \\alpha_n - H^{-1}(\\alpha_n)f^{'}(\\alpha_n) $$\n",
    "\n",
    "$$ \\beta_{i,v} ∝ \\sum_{d=1}^M \\sum_{n=1}^{N_d} \\phi_{d,n,i}1(v = w_{d,n}) $$\n",
    "\n",
    "\n",
    "\n",
    "$α$ 를 learning하는 과정 중 closed form solution을 만들기 어렵기 때문에 비선형 방정식의 해를 구할 때 사용되는 Newton-Raphson Method을 사용합니다.\n",
    "\n",
    "이 과정에서 $α$ 를 learning하는 데 쓰이는 gradient  $f'$와 Hessian Matrix $H$가 사용되는데 이들을 derive하는 과정은 다음과 같습니다.\n",
    "\n",
    "$$ f' = \\frac{d}{d\\alpha_i}L(\\gamma,\\phi|\\alpha,\\beta) = \\sum_{d=1}^{M} [-\\psi(\\sum_{i=1}^{K} \\gamma_{d,i}) + \\psi(\\gamma_{d,i}) + \\psi(\\sum_{i=1}^{K} \\alpha_i) - \\psi(\\alpha_i)]$$\n",
    "\n",
    "$$ H = \\frac{d}{d\\alpha_i\\alpha_j}L(\\gamma,\\phi|\\alpha,\\beta) = M\\psi'(\\sum_{i=1}^{K}\\alpha_i) - \\psi'(\\alpha_i)M1(i=j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.initial setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@ copyright: AAI lab (http://aailab.kaist.ac.kr/xe2/page_GBex27)\n",
    "@ author: Moon Il-chul: icmoon@kaist.ac.kr\n",
    "@ annotated by Na Byeong-hu: wp03052@kaist.ac.kr; Kim Kyu-Seok: kimkyu80@kaist.ac.kr; Bae Hee-sun: cat2507@kaist.ac.kr\n",
    "'''\n",
    "import time\n",
    "import numpy as np\n",
    "from numpy import ndarray, sum\n",
    "from scipy.special import polygamma, gamma as gammaFunc\n",
    "from math import log, e, exp\n",
    "import csv\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "intNumTopic = 5 # intNumTopic : topic 개수\n",
    "numIterations = 10 # numIterations : variational iteration 수\n",
    "numInternalIterations = 10 # numInternalIterations : Internal iteration 수\n",
    "numNewtonIteration = 5  # numNewtomIteration : Newton Iteration 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20newsgroup 데이터를 불러온다. data를 통해 corpusMatrix와 word 업데이트\n",
    "\n",
    "# training 데이터를 세팅하는 과정\n",
    "cats = ['comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x',\n",
    "        'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space'] # 해당 주제의 data만 사용\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',shuffle=True, random_state=0, categories=cats)\n",
    "\n",
    "bagVocab = np.load('bagVocab_3+_newstop_tfidf_2.npy')\n",
    "bagVocab_d = []\n",
    "for i in range(len(bagVocab)):\n",
    "    bagVocab_d.append(bagVocab[i].decode(\"utf-8\"))\n",
    "target = newsgroups_train.target  # 각 문서(뉴스)의 topic\n",
    "\n",
    "vect = CountVectorizer(vocabulary=bagVocab_d, binary=True)  # 단어를 count하는 방법 설정\n",
    "data = vect.fit_transform(newsgroups_train.data) # 각 문서별 단어 개수 세기 진행\n",
    "corpusMatrix = data.toarray().tolist()\n",
    "intNumDoc = len(corpusMatrix)  # 문서(뉴스)의 개수\n",
    "intUniqueWord = len(corpusMatrix[0])  # 단어모음 개수 = 2000개\n",
    "word = bagVocab_d  # 단어 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numWordPerDoc : Document 별 word 개수는 몇 개인지 업데이트할 것이다 \n",
    "numWordPerDoc = []\n",
    "\n",
    "for i in range(len(corpusMatrix)):\n",
    "    wordInDoc = corpusMatrix[i]\n",
    "    cnt = 0\n",
    "    for j in range(len(wordInDoc)):\n",
    "        cnt = cnt + wordInDoc[j]\n",
    "    numWordPerDoc.append(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpusList : ith document에 있는 unique word를 ith list에 저장할 것이다\n",
    "corpusList = []\n",
    "\n",
    "for i in range(len(corpusMatrix)):\n",
    "    listInCorpus = []\n",
    "    for j in range(len(corpusMatrix[0])):\n",
    "        if corpusMatrix[i][j] == 1:    #  ith document의 jth unique word가 있다면 해당 단어의 index를 corpusList의 ith list에 추가\n",
    "            listInCorpus.append(j)\n",
    "    corpusList.append(listInCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial alpha와 beta를 만들어준다.\n",
    "\n",
    "alpha = ndarray(shape=(intNumTopic),dtype=float)  # LDA의 hyperparameter, alpha\n",
    "beta = ndarray(shape=(intNumTopic,intUniqueWord),dtype=float)  # LDA의 hyperparameter, beta\n",
    "\n",
    "# alpha 초기값 업데이트\n",
    "for k in range(intNumTopic):\n",
    "    alpha[k] = 1.0 / float(intNumTopic)\n",
    "\n",
    "# beta 초기값 업데이트\n",
    "for k in range(intNumTopic):\n",
    "    for v in range(intUniqueWord):\n",
    "        beta[k][v] = 1.0 / float(intUniqueWord) + np.random.rand(1)*0.01\n",
    "    # beta normailize 진행\n",
    "    normalizeConstantBeta = sum(beta[k])\n",
    "    for v in range(intUniqueWord):\n",
    "        beta[k][v] = beta[k][v] / normalizeConstantBeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LDA performing\n",
    "\n",
    "LDA를 학습시킵니다. iteration 구조를 살펴보면,\n",
    "\n",
    "1) E-M iteration은 10번 진행하였으며 \n",
    "\n",
    "2) E-step iteration 즉 phi와 gamma를 정하는 iteration은 10번씩,\n",
    "\n",
    "3) Newton-Rhapson method iteration 즉 alpha를 정하는 iteration은 5번씩\n",
    "\n",
    "진행하는 것을 알 수 있습니다.\n",
    "\n",
    "그리고 각 EM iteration의 실행 시작 시간과 iteration 시작시 initial alpha를 사용해서 phi, gamma를 학습시켜 계산한 ELBO와 initial alpha, Newton-Rhapson method iteraion이 진행됨에 따라 변화하는 ELBO와 alpha를 출력합니다. 특히 Newton_rhapson method가 iteration하면서 ELBO는 계속 증가하는 경향성을 보이는 것을 알 수 있는데, 다만 전체 iteration 수가 적다보니 진행 중 ELBO가 감소하는 부분이 있습니다. 이 부분은 iteration을 더 많이 진행하게 되면 더 높은 ELBO가 나타나게 될 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loggammafunction을 정의한다.\n",
    "\n",
    "def loggammaFunc(num):\n",
    "    result = 0\n",
    "    if num < 170:\n",
    "        result = log(gammaFunc(num), e)\n",
    "    else:\n",
    "        result = log(num-1, e) + loggammaFunc(num-1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variational parameter phi, gamma의 초기값 setting\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "phi = []\n",
    "gamma = ndarray(shape=(intNumDoc, intNumTopic), dtype=float)\n",
    "\n",
    "# phi\n",
    "for d in range(intNumDoc):  # d번째 문서\n",
    "    phi.append(ndarray(shape=(numWordPerDoc[d], intNumTopic), dtype=float))\n",
    "    for n in range(numWordPerDoc[d]):  # d문서 안에서 n번째 단어\n",
    "        for k in range(intNumTopic): # 해당 단어가 각각의 topic일 확률\n",
    "            phi[d][n][k] = 1.0 / float(intNumTopic)\n",
    "\n",
    "# gamma\n",
    "for k in range(intNumTopic):  # topic k\n",
    "    for d in range(intNumDoc):  # d번째 문서\n",
    "        gamma[d][k] = alpha[k] + float(numWordPerDoc[d]) / float(intNumTopic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 1 (start time = 2.38)\n",
      "Variational Iteration : 1\n",
      "Newton-Rhapson Itr - ELBO : -2121351.5730551668 | alpha : [0.2 0.2 0.2 0.2 0.2]\n",
      "Newton-Rhapson Itr - ELBO : -2114403.7004250214 | alpha : [0.31902491 0.32938306 0.31993946 0.33554538 0.33786083]\n",
      "Newton-Rhapson Itr - ELBO : -2111106.376473788 | alpha : [0.45066437 0.4846413  0.45356134 0.50609305 0.51439773]\n",
      "Newton-Rhapson Itr - ELBO : -2110252.8222478293 | alpha : [0.54579097 0.60858885 0.55095568 0.65078525 0.66766018]\n",
      "Newton-Rhapson Itr - ELBO : -2110185.205575985 | alpha : [0.57754501 0.65452981 0.58374418 0.70828858 0.73025703]\n",
      "Newton-Rhapson Itr - ELBO : -2110184.674081642 | alpha : [0.58023569 0.65883918 0.58654274 0.71412916 0.73682905]\n",
      "Iteration : 2 (start time = 1419.07)\n",
      "Variational Iteration : 2\n",
      "Newton-Rhapson Itr - ELBO : -2102667.9831148903 | alpha : [0.58023569 0.65883918 0.58654274 0.71412916 0.73682905]\n",
      "Newton-Rhapson Itr - ELBO : -2100403.968694526 | alpha : [0.81029206 0.93802891 0.81061672 1.03048874 1.0657466 ]\n",
      "Newton-Rhapson Itr - ELBO : -2099941.9926887094 | alpha : [0.96528378 1.13262842 0.95906592 1.25561447 1.30051008]\n",
      "Newton-Rhapson Itr - ELBO : -2099919.8957662107 | alpha : [1.00758874 1.18734845 0.99902471 1.32004204 1.36784556]\n",
      "Newton-Rhapson Itr - ELBO : -2099919.835740504 | alpha : [1.00987666 1.1903941  1.00115935 1.32369026 1.37166621]\n",
      "Newton-Rhapson Itr - ELBO : -2099919.835740012 | alpha : [1.00988289 1.19040268 1.0011651  1.32370075 1.37167722]\n",
      "Iteration : 3 (start time = 2813.41)\n",
      "Variational Iteration : 3\n",
      "Newton-Rhapson Itr - ELBO : -2094706.4590028832 | alpha : [1.00988289 1.19040268 1.0011651  1.32370075 1.37167722]\n",
      "Newton-Rhapson Itr - ELBO : -2093050.097628397 | alpha : [1.38738517 1.64802969 1.36979047 1.84669552 1.91200764]\n",
      "Newton-Rhapson Itr - ELBO : -2092780.0198659783 | alpha : [1.61341768 1.92406114 1.58949839 2.1640927  2.23949041]\n",
      "Newton-Rhapson Itr - ELBO : -2092771.8307832878 | alpha : [1.66160575 1.9832053  1.63617682 2.23237958 2.30986996]\n",
      "Newton-Rhapson Itr - ELBO : -2092771.8220575715 | alpha : [1.66324114 1.98522143 1.63775604 2.23471571 2.31227517]\n",
      "Newton-Rhapson Itr - ELBO : -2092771.8220575594 | alpha : [1.66324291 1.98522363 1.63775774 2.23471826 2.3122778 ]\n",
      "Iteration : 4 (start time = 4204.57)\n",
      "Variational Iteration : 4\n",
      "Newton-Rhapson Itr - ELBO : -2088113.8057152596 | alpha : [1.66324291 1.98522363 1.63775774 2.23471826 2.3122778 ]\n",
      "Newton-Rhapson Itr - ELBO : -2087080.928157835 | alpha : [2.1943111  2.61365865 2.14885498 2.96042599 3.07042216]\n",
      "Newton-Rhapson Itr - ELBO : -2086969.2615989766 | alpha : [2.44121627 2.90505107 2.38545471 3.29797352 3.4234686 ]\n",
      "Newton-Rhapson Itr - ELBO : -2086967.7549399508 | alpha : [2.47435836 2.94408102 2.41711442 3.3432789  3.47089114]\n",
      "Newton-Rhapson Itr - ELBO : -2086967.7546319615 | alpha : [2.47484166 2.94464902 2.41757476 3.34393944 3.47158304]\n",
      "Newton-Rhapson Itr - ELBO : -2086967.754631955 | alpha : [2.47484176 2.94464914 2.41757485 3.34393958 3.47158318]\n",
      "Iteration : 5 (start time = 5598.43)\n",
      "Variational Iteration : 5\n",
      "Newton-Rhapson Itr - ELBO : -2083076.5085484756 | alpha : [2.47484176 2.94464914 2.41757485 3.34393958 3.47158318]\n",
      "Newton-Rhapson Itr - ELBO : -2082600.3155241485 | alpha : [3.04912157 3.5973356  2.94871668 4.1078156  4.28597785]\n",
      "Newton-Rhapson Itr - ELBO : -2082575.0453661205 | alpha : [3.22153642 3.79206662 3.10705771 4.33626554 4.53015199]\n",
      "Newton-Rhapson Itr - ELBO : -2082574.9625312164 | alpha : [3.23219004 3.80404062 3.1167883  4.35033714 4.54522044]\n",
      "Newton-Rhapson Itr - ELBO : -2082574.962530265 | alpha : [3.23222641 3.8040813  3.11682133 4.35038502 4.54527181]\n",
      "Newton-Rhapson Itr - ELBO : -2082574.9625302628 | alpha : [3.23222641 3.8040813  3.11682133 4.35038503 4.54527182]\n",
      "Iteration : 6 (start time = 7021.72)\n",
      "Variational Iteration : 6\n",
      "Newton-Rhapson Itr - ELBO : -2079337.226119746 | alpha : [3.23222641 3.8040813  3.11682133 4.35038503 4.54527182]\n",
      "Newton-Rhapson Itr - ELBO : -2079186.6236504868 | alpha : [3.67956082 4.28258044 3.49832878 4.92497395 5.17596647]\n",
      "Newton-Rhapson Itr - ELBO : -2079183.887686428 | alpha : [3.75091107 4.35823687 3.55853483 5.01603749 5.27631495]\n",
      "Newton-Rhapson Itr - ELBO : -2079183.8866630949 | alpha : [3.7523311  4.35973143 3.55972234 5.0178397  5.27830747]\n",
      "Newton-Rhapson Itr - ELBO : -2079183.8866630911 | alpha : [3.75233164 4.35973199 3.55972279 5.01784038 5.27830822]\n",
      "Newton-Rhapson Itr - ELBO : -2079183.886663106 | alpha : [3.75233164 4.35973199 3.55972279 5.01784038 5.27830822]\n",
      "Iteration : 7 (start time = 8407.63)\n",
      "Variational Iteration : 7\n",
      "Newton-Rhapson Itr - ELBO : -2076397.9330568486 | alpha : [3.75233164 4.35973199 3.55972279 5.01784038 5.27830822]\n",
      "Newton-Rhapson Itr - ELBO : -2076375.6599054183 | alpha : [3.95711543 4.53957535 3.6865387  5.25805162 5.56763903]\n",
      "Newton-Rhapson Itr - ELBO : -2076375.5948238685 | alpha : [3.96907378 4.55017283 3.69421306 5.27201217 5.58450826]\n",
      "Newton-Rhapson Itr - ELBO : -2076375.5948232836 | alpha : [3.96911025 4.5502053  3.69423695 5.27205451 5.58455962]\n",
      "Newton-Rhapson Itr - ELBO : -2076375.5948232613 | alpha : [3.96911025 4.5502053  3.69423695 5.27205451 5.58455962]\n",
      "Newton-Rhapson Itr - ELBO : -2076375.5948232585 | alpha : [3.96911025 4.5502053  3.69423695 5.27205451 5.58455962]\n",
      "Iteration : 8 (start time = 9803.14)\n",
      "Variational Iteration : 8\n",
      "Newton-Rhapson Itr - ELBO : -2073847.7073423148 | alpha : [3.96911025 4.5502053  3.69423695 5.27205451 5.58455962]\n",
      "Newton-Rhapson Itr - ELBO : -2073836.8182988295 | alpha : [3.9068962  4.41246179 3.55452913 5.15287774 5.50123224]\n",
      "Newton-Rhapson Itr - ELBO : -2073836.80372048 | alpha : [3.91012913 4.41783354 3.55990438 5.15799908 5.505835  ]\n",
      "Newton-Rhapson Itr - ELBO : -2073836.803720454 | alpha : [3.9101342  4.41784127 3.55991223 5.15800672 5.50584231]\n",
      "Newton-Rhapson Itr - ELBO : -2073836.8037204656 | alpha : [3.9101342  4.41784127 3.55991223 5.15800672 5.50584231]\n",
      "Newton-Rhapson Itr - ELBO : -2073836.8037204656 | alpha : [3.9101342  4.41784127 3.55991223 5.15800672 5.50584231]\n",
      "Iteration : 9 (start time = 11224.34)\n",
      "Variational Iteration : 9\n",
      "Newton-Rhapson Itr - ELBO : -2071423.199051941 | alpha : [3.9101342  4.41784127 3.55991223 5.15800672 5.50584231]\n",
      "Newton-Rhapson Itr - ELBO : -2071365.5241278978 | alpha : [3.62651724 4.02293008 3.20938959 4.74507497 5.11339606]\n",
      "Newton-Rhapson Itr - ELBO : -2071364.975385972 | alpha : [3.65058161 4.05629038 3.23942596 4.77995015 5.14701975]\n",
      "Newton-Rhapson Itr - ELBO : -2071364.9753432882 | alpha : [3.65079531 4.05658467 3.2396962  4.78025812 5.14732132]\n",
      "Newton-Rhapson Itr - ELBO : -2071364.9753432984 | alpha : [3.65079533 4.05658469 3.23969622 4.78025814 5.14732134]\n",
      "Newton-Rhapson Itr - ELBO : -2071364.975343298 | alpha : [3.65079533 4.05658469 3.23969622 4.78025814 5.14732134]\n",
      "Iteration : 10 (start time = 12615.21)\n",
      "Variational Iteration : 10\n",
      "Newton-Rhapson Itr - ELBO : -2068982.9289592174 | alpha : [3.65079533 4.05658469 3.23969622 4.78025814 5.14732134]\n",
      "Newton-Rhapson Itr - ELBO : -2068858.9314073594 | alpha : [3.22804939 3.50609806 2.76654248 4.18674718 4.55935743]\n",
      "Newton-Rhapson Itr - ELBO : -2068856.169569104 | alpha : [3.27852581 3.57210374 2.823998   4.25765458 4.62996793]\n",
      "Newton-Rhapson Itr - ELBO : -2068856.1685066116 | alpha : [3.27952734 3.57341856 2.82516565 4.25906163 4.63137988]\n",
      "Newton-Rhapson Itr - ELBO : -2068856.168506614 | alpha : [3.27952772 3.57341907 2.82516611 4.25906217 4.63138043]\n",
      "Newton-Rhapson Itr - ELBO : -2068856.1685066135 | alpha : [3.27952772 3.57341907 2.82516611 4.25906217 4.63138043]\n"
     ]
    }
   ],
   "source": [
    "# LDA를 perform한다.\n",
    "    \n",
    "for iteration in range(numIterations):\n",
    "    \n",
    "    print ('Iteration : ' + str(iteration+1) + ' (start time = ' + str(round(time.time()-start_time,2)) + ')')\n",
    "\n",
    "    print('Variational Iteration : '+str(iteration+1))\n",
    "\n",
    "    ##### E-Step : phi와 gamma (variational parameter) 업데이트\n",
    "    for iterationInternal in range(numInternalIterations):\n",
    "        \n",
    "        ### phi 학습\n",
    "        for d in range(intNumDoc):  # 문서\n",
    "            for n in range(numWordPerDoc[d]):  # 문서 내 단어\n",
    "                for k in range(intNumTopic):  # topic\n",
    "                    # phi = beta * exp(polygamma(gamma))\n",
    "                    phi[d][n][k] = beta[k][corpusList[d][n]]*exp(polygamma(0,gamma[d][k]))\n",
    "                # normalize phi : 단어 하나가 모든 topic에 대해 가지는 확률의 총합은 1이다\n",
    "                normalizeConstantPhi = sum(phi[d][n])\n",
    "                for k in range(intNumTopic):\n",
    "                    phi[d][n][k] = float(phi[d][n][k]) / normalizeConstantPhi\n",
    "        \n",
    "        ### gamma 학습\n",
    "        # gamma = alpha + phi\n",
    "        for d in range(intNumDoc):  # 문서\n",
    "            for k in range(intNumTopic):  # topic k \n",
    "                gamma[d][k] = alpha[k]\n",
    "                for n in range(numWordPerDoc[d]):  # phi는 단어 따라 다르므로 단어에 대한 iteration\n",
    "                    gamma[d][k] = gamma[d][k] + phi[d][n][k]\n",
    "        \n",
    "    ##### M-Step : alpha와 beta (model parameter) 업데이트\n",
    "\n",
    "    ### Beta 학습\n",
    "    for k in range(intNumTopic):  # topic k\n",
    "        for v in range(intUniqueWord):  # 단어모음 2000개 index\n",
    "            beta[k][v] = 0\n",
    "    for k in range(intNumTopic):  # topic k\n",
    "        for d in range(intNumDoc):  # 문서 d\n",
    "            for n in range(numWordPerDoc[d]):  # 문서 내 단어 n\n",
    "                beta[k][corpusList[d][n]] += phi[d][n][k]\n",
    "        # normalize beta : 한 단어가 모든 topic에 대해 가지는 확률의 총합은 1이다\n",
    "        normalizeConstantBeta = sum(beta[k]) \n",
    "        for v in range(intUniqueWord):\n",
    "            beta[k][v] = beta[k][v] / normalizeConstantBeta\n",
    "\n",
    "    ### Alpha 학습 (Newton-Rhapson Iteration 필요)\n",
    "    \n",
    "    ## 현재 alpha를 이용한 ELBO 계산\n",
    "    ELBOMax = 0\n",
    "    # 1) E_q(logP(theta|alpha)) = sum_{d}(loggamma(sum(alpha))+sum_{k}((alpha-1)(-polygamma(sum(gamma))+polygamma(gamma))-loggamma(alpha)))\n",
    "    for d in range(intNumDoc):  # 문서 d\n",
    "        ELBOMax += loggammaFunc(sum(alpha))\n",
    "        for k in range(intNumTopic):  # topic k\n",
    "            ELBOMax -= loggammaFunc(alpha[k])\n",
    "            ELBOMax += (alpha[k] - 1) * (polygamma(0, gamma[d][k]) - polygamma(0, sum(gamma[d])))\n",
    "    \n",
    "    # 2) E_q(logP(z|theta)) = sum_{d}sum_{n}sum_{k}(phi*(-polygamma(sum(gamma))+polygamma(gamma)))\n",
    "    for d in range(intNumDoc):\n",
    "        for n in range(numWordPerDoc[d]):\n",
    "            for k in range(intNumTopic):\n",
    "                ELBOMax += phi[d][n][k] * (polygamma(0, gamma[d][k]) - polygamma(0, sum(gamma[d])))\n",
    "    \n",
    "    # 3) E_q(logP(w|z, beta)) = sum_{d}sum_{n}sum_{k}(phi*log(beta))\n",
    "    for d in range(intNumDoc):\n",
    "        for n in range(numWordPerDoc[d]):\n",
    "            for k in range(intNumTopic):\n",
    "                ELBOMax += phi[d][n][k] * log(beta[k][corpusList[d][n]], e)\n",
    "    \n",
    "    # 4) H(q) = sum_{d}sum{k}(-(gamma-1)(-polygamma(sum(gamma))+polygamma(gamma))+loggamma(gamma))-sum_{d}(loggamma(sum(gamma)))\n",
    "    #             -sum_{d}sum_{k}sum{n}(phi*log(phi))\n",
    "    for d in range(intNumDoc):\n",
    "        ELBOMax -= loggammaFunc(sum(gamma[d]))\n",
    "        for k in range(intNumTopic):\n",
    "            ELBOMax += loggammaFunc(gamma[d][k])\n",
    "            ELBOMax -= (gamma[d][k] - 1) * (polygamma(0, gamma[d][k]) - polygamma(0, sum(gamma[d])))\n",
    "            for n in range(numWordPerDoc[d]):\n",
    "                ELBOMax -= phi[d][n][k] * log(phi[d][n][k])\n",
    "    \n",
    "    print( 'Newton-Rhapson Itr - ELBO : '+str(ELBOMax)+' | alpha : '+str(alpha) )\n",
    "\n",
    "    ## Newton-Rhapson optimization\n",
    "    for itr in range(numNewtonIteration):\n",
    "        # Hessian Matrix와 Derivative Vector 계산\n",
    "        H = ndarray(shape=(intNumTopic, intNumTopic), dtype=float)\n",
    "        g = ndarray(shape=(intNumTopic), dtype=float)\n",
    "        for k1 in range(intNumTopic):\n",
    "            g[k1] = float(intNumDoc)*(polygamma(0,sum(alpha))-polygamma(0,alpha[k1]))\n",
    "            for d in range(intNumDoc):\n",
    "                g[k1] += ( polygamma(0,gamma[d][k1]) - polygamma(0,sum(gamma[d])) )\n",
    "            for k2 in range(intNumTopic):\n",
    "                H[k1][k2] = 0\n",
    "                if k1 == k2:\n",
    "                    H[k1][k2] = H[k1][k2] - float(intNumDoc) * polygamma(1,alpha[k1])\n",
    "                H[k1][k2] = H[k1][k2] + float(intNumDoc) * polygamma(1,sum(alpha))\n",
    "\n",
    "        ## Alpha 업데이트\n",
    "        # delta Alpha = inverse(Hessian(alpha)) * f'(alpha)\n",
    "        deltaAlpha = np.dot(np.linalg.inv(H),g)\n",
    "\n",
    "        for k in range(intNumTopic):\n",
    "            alpha[k] = alpha[k] - deltaAlpha[k]\n",
    "            \n",
    "            if alpha[k] < 0.00001: # numerical error 방지를 위해\n",
    "                alpha[k] = 0.00001\n",
    "\n",
    "        ## 새로운 alpha를 이용한 ELBO 계산\n",
    "        ELBOAfter = 0\n",
    "        #1) E_q(logP(theta|alpha)) = sum_{d}(loggamma(sum(alpha))+sum_{k}((alpha-1)(-polygamma(sum(gamma))+polygamma(gamma))-loggamma(alpha)))\n",
    "        for d in range(intNumDoc):\n",
    "            ELBOAfter += loggammaFunc(sum(alpha))\n",
    "            for k in range(intNumTopic):\n",
    "                ELBOAfter -= loggammaFunc(alpha[k])\n",
    "                ELBOAfter += (alpha[k] - 1) * (polygamma(0, gamma[d][k]) - polygamma(0, sum(gamma[d])))\n",
    "        \n",
    "        #2) E_q(logP(z|theta)) = sum_{d}sum_{n}sum_{k}(phi*(-polygamma(sum(gamma))+polygamma(gamma)))\n",
    "        for d in range(intNumDoc):\n",
    "            for n in range(numWordPerDoc[d]):\n",
    "                for k in range(intNumTopic):\n",
    "                    ELBOAfter += phi[d][n][k] * (polygamma(0, gamma[d][k]) - polygamma(0, sum(gamma[d])))\n",
    "        \n",
    "        #3) E_q(logP(w|z, beta))= sum_{d}sum_{n}sum_{k}(phi*log(beta))\n",
    "        for d in range(intNumDoc):\n",
    "            for n in range(numWordPerDoc[d]):\n",
    "                for k in range(intNumTopic):\n",
    "                    ELBOAfter += phi[d][n][k] * log(beta[k][corpusList[d][n]], e)\n",
    "        \n",
    "        # 4) H(q) = sum_{d}sum{k}(-(gamma-1)(-polygamma(sum(gamma))+polygamma(gamma))+loggamma(gamma))-sum_{d}(loggamma(sum(gamma)))\n",
    "        #             -sum_{d}sum_{k}sum{n}(phi*log(phi))\n",
    "        for d in range(intNumDoc):\n",
    "            ELBOAfter -= loggammaFunc(sum(gamma[d]))\n",
    "            for k in range(intNumTopic):\n",
    "                ELBOAfter += loggammaFunc(gamma[d][k])\n",
    "                ELBOAfter -= (gamma[d][k] - 1) * (polygamma(0, gamma[d][k]) - polygamma(0, sum(gamma[d])))\n",
    "                for n in range(numWordPerDoc[d]):\n",
    "                    ELBOAfter -= phi[d][n][k] * log(phi[d][n][k])\n",
    "\n",
    "        print('Newton-Rhapson Itr - ELBO : '+str(ELBOAfter)+' | alpha : '+str(alpha))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top  20  Words in Each Topic\n",
      "\n",
      "Topic  1 :  systems, possible, question, set, read, following, space, window, user, code, support, apple, corporation, application, disk, disclaimer, group, note, questions, internet\n",
      "Topic  2 :  software, key, public, science, program, version, phone, available, message, clipper, david, number, encryption, government, email, chip, fax, thanks, keys, non\n",
      "Topic  3 :  team, win, play, game, keywords, canada, hockey, cmu, pittsburgh, teams, school, andrew, toronto, contact, thanks, post, center, info, april, games\n",
      "Topic  4 :  best, thanks, got, didn, actually, advance, better, thought, seen, quite, big, news, thing, away, come, ago, left, yes, anybody, won\n",
      "Topic  5 :  usa, net, access, high, doesn, lot, sure, car, things, look, buy, drive, bit, better, different, maybe, thing, old, thanks, able\n"
     ]
    }
   ],
   "source": [
    "# 5 topic 학습시켜서 topic별 상위 20개 단어를 보여준다\n",
    "\n",
    "arrMat = np.array(beta)\n",
    "arrWord = np.array(word)\n",
    "\n",
    "num_words = 20\n",
    "\n",
    "print(\"Top \",'%2d'%(num_words), \" Words in Each Topic\\n\")\n",
    "\n",
    "for i in range(intNumTopic):\n",
    "    index = (-arrMat[i]).argsort()\n",
    "    top20idx = index[:num_words]\n",
    "    s = 'Topic %2d : '%(i + 1)\n",
    "    for word in arrWord[top20idx]:\n",
    "        s += ' %s,' % (word)\n",
    "    print(s[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과해석부\n",
    "위의 결과는 20NewsGroups 데이터셋을 5개 topic의 Variational-LDA로 학습한 결과를 나타냅니다. 20NewsGroups 데이터셋은 약 18,000개의 newsgroups 게시글을 20개의 토픽으로 분류해 놓은 데이터셋으로 11,314개의 training 게시글과 1,073개의 testing 게시글로 이루어져 있습니다.\n",
    "\n",
    "모델을 통해 학습된 각 주제의 상위 20개 단어를 나타냈습니다. beta matrix의 각 row가 주제가 되고 column이 단어가 되어 row별로 beta값이 큰 단어를 차례대로 나타낸 것입니다. iteration 수가 적어 명확하게 단어가 구분되고 있지는 않지만, 그럼에도 topic 3에서 주로 스포츠에 관한 단어들이 대표 단어로 추출되었음을 알 수 있습니다. 이를 통해 topic 3이 20NewsGroups 데이터셋의 토픽 중 'rec.sport'에 관한 기사들을 잘 모은 것을 알 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
