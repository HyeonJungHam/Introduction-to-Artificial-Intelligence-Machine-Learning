{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qSfCPlDqlftY"
   },
   "source": [
    "# Neural Variational Document Model\n",
    "\n",
    "###  <div align=center> Moon Il-chul(icmoon@kaist.ac.kr); Na Byeong-hu(wp03052@kaist.ac.kr); Bae Hee-sun(cat2507@kaist.ac.kr) </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLUTPjjllhhu"
   },
   "source": [
    "**본 코드는  VAE의 인공신경망 구조 (hidden layer의 개수 등)을 수정한 것으로 VAE 코드와 유사합니다.**\n",
    "\n",
    "본 코드는 20 newsgroups 데이터셋을 이용한 NVDM(Neural Variational Document Model) 구현 예시입니다.\n",
    "\n",
    "본 코드를 통해서 NVDM의 구조를 익힐 수 있습니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uYfpViTeqT9i"
   },
   "source": [
    "## Neural Variational Inference Framework\n",
    "\n",
    "본 모델에서는 latent variable h 가 있는 generative model을 정의합니다. 이는 deep neural network의 stochastic unit으로 고려될 수 있습니다. h의 parent와 child를 각각 x와 y로 정의한다면 generative model의 joint distribution은 다음과 같이 정의할 수 있습니다.\n",
    "\n",
    "$p_{\\theta}(x,y) = \\sum_{h}p_{\\theta}(y \\mid h)p_{\\theta}(h \\mid x) p(x) = \\sum_{h}p_{\\theta}(x,y,h)$\n",
    "\n",
    "이 때 $logp$의 lower bound를 L이라고 한다면, \n",
    "\n",
    "$logp_{\\theta}(x,y)$ = $log\\sum_{h}p_{\\theta}(y \\mid h)p_{\\theta}(h \\mid x) p(x)$ = $log\\sum_{h}p_{\\theta}(y \\mid h)p_{\\theta}(h \\mid x) p(x) \\frac{q(h)}{q(h)}$ >= $\\sum_{h}  q(h)log \\frac{p_{\\theta}(y \\mid h)p_{\\theta}(h \\mid x) p(x)}{q(h)}$ = $\\sum_{h} q(h) [log p_{\\theta}(y \\mid h)p_{\\theta}(h \\mid x) p(x)-logq(h)] $ 이므로, 정리하면 $L = E_{q(h)}[logp_{\\theta}(y \\mid h)p_{\\theta}(h\\mid x)p(x)-logq(h)]$로 표현할 수 있습니다.\n",
    "\n",
    "또한, $logp_{\\theta}(x,y)$ = $log\\sum_{h}p_{\\theta}(x,y,h) \\frac{q(h)}{q(h)}$ >= $\\sum_{h}q(h)log\\frac{p_{\\theta}(x,y,h)}{q(h)}$ = $\\sum_{h}q(h)logp_{\\theta}(x,y,h)-q(h)logq(h)$ = $\\sum_{h}q(h)logp_(x,y)p_(h\\mid x,y)-q(h)logq(h)$ = $\\sum_{h}q(h)logp_(x,y)-q(h)\\frac{logq(h)}{logp_(h\\mid x,y)} $ 이므로, q(h)가 $p(h\\mid x,y) $와 최대한 유사할 때 lower bound L이 최대한 tight하다는 것을 알 수 있습니다.\n",
    "\n",
    "$q_{\\phi}(h\\mid x,y) $를 diagonal Gaussian distribution이라고 하면, $\\mu$와 $\\sigma$는 x,y의 function입니다.\n",
    "h ~ $q_{\\phi}(h\\mid x,y)$로부터 lower bound를 optimize하는 back-propagation을 힐 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3MVFiW8s8Y_V"
   },
   "source": [
    "## Neural Variational Document Model\n",
    "\n",
    "NVDM(Neural Variational Document Model)은 text의 unsupervised generative model로, 각 document에서 continuous semantic latent variable을 추출하는 것을 목표로 합니다. 이 모델은 VAE(Variational Auto-Encoder)로 해석될수 있는데, encoder가 bag-of-words document를 continuous latent distribution으로 compress하고, decoder가 softmax를 이용해 document의 모든words를 independently generate함으로써 document를 reconstruct합니다. \n",
    "구체적으로, X를 document의 bag-of-words representation, $x_{i}$를 position i의 word의 one-hot representation이라고 하고,\n",
    "$q(h\\mid X)$ 가 X를 hidden variable h로 compress하며(encoder), $p(X\\mid h)$가 h를 $x_{i}$로 reconstruct한다고 하겠습니다(decoder).\n",
    "\n",
    "Log-likelihood 즉, $log\\sum_{h}p(X\\mid h)p(h) $를 maximize하기 위해, lower bound를 보이자면,\n",
    "$L=E_{q_{\\phi}(h\\mid X)}[\\sum_{i}logp_{\\theta}(x_{i}\\mid h)]-D_{KL}[q_{\\phi}(h\\mid X)||p(h)]$ 입니다. 이 때, $p_{\\theta}(x_{i}\\mid h)$는 multinomial logistic regression으로 modeling되며 다음과 같이 표현될 수 있습니다.\n",
    "\n",
    "$p_{\\theta}(x_{i}\\mid h)=\\frac{exp[-E(x_{i};h,\\theta)]}{\\sum_{j} exp[-E(x_{j};h,\\theta)]}$\n",
    "\n",
    "$E(x_{i};h,\\theta)=-h^T Rx_{i}-b_{x_{i}} $\n",
    "R matrix는 decoder의 weight라고 생각할 수 있는데, semantic word embedding을 learn합니다.\n",
    "\n",
    "즉 NVDM은 VAE와 구조적으로 유사하다 할 수 있으나, VAE의 decoding 과정에서 encoder를 대칭시킨 것 같은 decoder를 사용하는 것과는 다르게 softmax를 이용하여 class를 구분하여 확률을 결과로 내놓는 것에서 그 차이가 있다고 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ceU9W5Fclfta"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BkX-hhHPlftf"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@ copyright: AAI lab (http://aailab.kaist.ac.kr/xe2/page_GBex27)\n",
    "@ author: Moon Il-chul: icmoon@kaist.ac.kr\n",
    "@ annotated by Na Byeong-hu: wp03052@kaist.ac.kr; Bae Hee-sun: cat2507@kaist.ac.kr\n",
    "'''\n",
    "\n",
    "class NVDM:\n",
    "\n",
    "    network_architecture = [] # network_architecture : 인공신경망(Neural Network)의 구조를 저장\n",
    "    transfer_fct = 0 #transfer_fct : 인공신경망의 transfer function을 결정\n",
    "\n",
    "    x = [] # x : NVDM의 input, bag-of-words\n",
    "    pb_x = [] # pb_x : NVDM의 output, latent variable로부터 어떤 문서에 각 단어가 존재할 확률을 나타낸다. softmax의 결과.\n",
    "    z = [] # z : NVDM의 latent variable\n",
    "    z_mean = [] # z_mean : NVDM의 latent variable의 mean\n",
    "    z_log_sigma_sq = [] # z_log_sigma_sq : NVDM의 latent variable의 log sigma sq\n",
    "\n",
    "    sess = 0\n",
    "    cost = 0\n",
    "    optimizer = 0\n",
    "\n",
    "    def __init__(self,network_architecture,transfer_fct=tf.nn.relu):\n",
    "        self.network_architecture = network_architecture\n",
    "        self.transfer_fct = transfer_fct # activation function을 저장한다. 기본값은 relu function\n",
    "\n",
    "        self.x = tf.placeholder(tf.float32,[None,network_architecture[\"n_input\"]]) # input. 각 문서에 해당 단어(2000)가 몇 개 들어있는지.\n",
    "\n",
    "    def create_network(self,batch_size):\n",
    "        # NVDM을 생성하는 method\n",
    "        # initialize weight -> recognition network -> Reparameterization trick -> generator network\n",
    "        \n",
    "        network_weights = self.initialize_weights(**self.network_architecture)\n",
    "    \n",
    "        self.z_mean, self.z_log_sigma_sq = self.recognition_network(network_weights[\"weights_recog\"],network_weights[\"biases_recog\"])\n",
    "\n",
    "        n_z = self.network_architecture[\"n_z\"]\n",
    "        \n",
    "        # Reparameteriation trick\n",
    "        eps = tf.random_normal((batch_size,n_z),0,1,dtype=tf.float32)\n",
    "\n",
    "        self.z = tf.add(self.z_mean,tf.multiply(tf.sqrt(tf.exp(self.z_log_sigma_sq)),eps))\n",
    "        \n",
    "        self.R = network_weights[\"weights_gener\"][\"R\"]\n",
    "\n",
    "        self.pb_x = self.generator_network(network_weights[\"weights_gener\"],network_weights[\"biases_gener\"])\n",
    "\n",
    "    def initialize_weights(self, n_hidden_recog_1, n_hidden_recog_2, n_input, n_z):\n",
    "        # NVDM의 전체 weights의 초기값을 설정하여 dictionary로 return하는 method\n",
    "        # VAE와의 차이점 : generator network의 hidden layer가 없어 n_hidden_gener가 없음\n",
    "        # Input : n_hidden_recog_1, n_hidden_recog_2, n_input, n_z\n",
    "        # Output : all_weights\n",
    "        \n",
    "        all_weights = dict()\n",
    "        # Recognition Network의 weight의 초기값 설정 with Xavier initialization\n",
    "        all_weights['weights_recog'] = { 'h1' : tf.Variable(self.xavier_init(n_input,n_hidden_recog_1)),\\\n",
    "                                        'h2' : tf.Variable(self.xavier_init(n_hidden_recog_1,n_hidden_recog_2)),\\\n",
    "                                        'out_mean': tf.Variable(self.xavier_init(n_hidden_recog_2, n_z)),\\\n",
    "                                        'out_log_sigma': tf.Variable(self.xavier_init(n_hidden_recog_2, n_z))}\n",
    "\n",
    "        # Recognition Network의 bias의 초기값 설정 with 0\n",
    "        all_weights['biases_recog'] = { 'b1' : tf.Variable(tf.zeros([n_hidden_recog_1],dtype=tf.float32)),\\\n",
    "                                       'b2' : tf.Variable(tf.zeros([n_hidden_recog_2],dtype=tf.float32)),\\\n",
    "                                       'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\\\n",
    "                                       'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\n",
    "        \n",
    "        # Generator Network의 weight의 초기값 설정 with Xavier initialization\n",
    "        all_weights['weights_gener'] = {'R': tf.Variable(self.xavier_init(n_z, n_input))}\n",
    "        \n",
    "        # Generator Network의 bias의 초기값 설정 with 0\n",
    "        all_weights['biases_gener'] = {'b': tf.Variable(tf.zeros([n_input], dtype=tf.float32))}\n",
    "        return all_weights\n",
    "\n",
    "    def recognition_network(self,weights,biases):\n",
    "        # NVDM의 recognition network의 역할을 수행하는 method\n",
    "        # Input : (recognition network의) weights, biases\n",
    "        # Output: (z_mean, z_log_sigma_sq)\n",
    "        \n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']), biases['b1'])) # L1 = g(b1 + W1*X)\n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])) # L2 = g(b2 + W2*L1)\n",
    "        z_mean = tf.add(tf.matmul(layer_2,weights['out_mean']),biases['out_mean']) # z_mean = b_{out_mean} + W_{out mean}*L2\n",
    "        z_log_sigma_sq = tf.add(tf.matmul(layer_2,weights['out_log_sigma']),biases['out_log_sigma'])\n",
    "        # z_log_sigma_sq = b_{out_log_sigma} + W_{out_log_sigma}*L2\n",
    "        return (z_mean,z_log_sigma_sq)\n",
    "\n",
    "    def generator_network(self,weights,biases):\n",
    "        # NVDM의 generator network의 역할을 수행하는 method\n",
    "        # VAE와의 차이점 : output이 x를 reconstruct하지 않고 softmax를 통해 확률로 출력\n",
    "        # Input : (generator network의) weights, biases\n",
    "        # Output : pb_x\n",
    "        \n",
    "        pb_x = tf.nn.softmax(tf.add(tf.matmul(self.z, weights['R']), biases['b']))\n",
    "        # pb_x = softmax(b + R*Z)\n",
    "        \n",
    "        return pb_x\n",
    "\n",
    "    def create_loss_optimizer(self,learning_rate):\n",
    "        # NVDM의 total cost를 계산하고, 이를 minimize하는 optimizer를 설정하는 메소드\n",
    "        # Input : learning_rate\n",
    "        # Output : None\n",
    "        \n",
    "        # reconstruction loss function (문서 내에 있는 단어들의 log_pb을 더해야하기 때문에 self.x를 곱함)\n",
    "        reconstr_loss = -tf.reduce_sum(tf.multiply(tf.log(self.pb_x),self.x),1)\n",
    "        \n",
    "        # latent loss function = KL-divergence\n",
    "        latent_loss = -0.5*tf.reduce_sum(1+self.z_log_sigma_sq-tf.square(self.z_mean)-tf.exp(self.z_log_sigma_sq),1)\n",
    "        \n",
    "        # (NVDM total) cost = reconstruction loss + latent loss\n",
    "        self.cost = tf.reduce_mean(reconstr_loss+latent_loss)\n",
    "        \n",
    "        # optimizer 설정 (Adam Optimizer 사용)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost)\n",
    "\n",
    "    def partial_fit(self,x):\n",
    "        # NVDM을 통하여 학습한 Input data x의 cost를 출력하는 메소드 \n",
    "        # Input : x\n",
    "        # Output : cost\n",
    "        \n",
    "        opt,cost = self.sess.run((self.optimizer,self.cost),feed_dict={self.x:x})\n",
    "        return cost\n",
    "\n",
    "    def transform(self,x):\n",
    "        # NVDM의 encoder network를 통하여 latent variable z의 mean을 return하는 메소드\n",
    "        # Input : x\n",
    "        # Output : z_mean\n",
    "        return self.sess.run(self.z_mean,feed_dict={self.x:x})\n",
    "\n",
    "    def generate(self,z_mean=None):\n",
    "        # NVDM의 decoder network를 통하여 output인 pb_x를 return하는 메소드\n",
    "        # Input : z_mean\n",
    "        # Output : pb_x\n",
    "        if z_mean is None:\n",
    "            z_mean = np.random.normal(size=self.network_architecture[\"n_z\"])\n",
    "        return self.sess.run(self.pb_x,feed_dict={self.z:z_mean})\n",
    "\n",
    "    def reconstruct(self,x):\n",
    "        # NVDM을 통하여 X를 reconstruct한 후, reconstruct pb_x를 return하는 메소드\n",
    "        # Input : x\n",
    "        # Output : (NVDM을 통한 reconstruct) pb_x\n",
    "        return self.sess.run(self.pb_x,feed_dict={self.x:x})\n",
    "\n",
    "    def xavier_init(self,fan_in, fan_out, constant=1): \n",
    "        # Xavier initialization method\n",
    "        # Input : fan_in(들어오는 layer의 크기), fan_out(나가는 layer의 크기)\n",
    "        # Output : 각 원소가 random uniform value인 (fan_in, fan_out) 형태의 tensor \n",
    "        low = -constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "        high = constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "\n",
    "        return tf.random_uniform((fan_in, fan_out), minval=low, maxval=high, dtype=tf.float32)\n",
    "\n",
    "    def train(self,trainX,batch_size=100,training_epochs=500,learning_rate=0.0005):\n",
    "        # NVDM를 생성하고, trainX를 training하는 메소드\n",
    "        # Input : trainX, batch_size, training_epochs, learning_rate\n",
    "        # Output : None\n",
    "        \n",
    "        total_costs = np.zeros(training_epochs) # epoch당 cost를 저장할 array \n",
    "        total_perplexity = np.zeros(training_epochs) # epoch당 perplexity를 저장할 array\n",
    "\n",
    "        self.create_network(batch_size) # NVDM 생성\n",
    "        self.create_loss_optimizer(learning_rate) # NVDM의 loss function과 optimizer 생성\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "        # session : tensorflow를 올려놓아 실행을 진행하기 위한 공간\n",
    "        self.sess.run(init)\n",
    "\n",
    "        startTime = time.time()\n",
    "        for epoch in range(training_epochs): # training_epochs만큼 반복 실행\n",
    "            avg_cost = 0. # training set의 average cost를 저장\n",
    "            avg_perplexity = 0. # training set의 average perplexity를 저장\n",
    "            total_batch = int(len(trainX)/batch_size) # 전체 batch의 갯수\n",
    "\n",
    "            for i in range(total_batch): # total_batch만큼 반복 실행\n",
    "                batch = []\n",
    "                for j in range(batch_size): # trainX에서 i번째 batch를 가져오는 과정\n",
    "                    batch.append(trainX[i*batch_size+j])\n",
    "                cost = self.partial_fit(batch) # i번째 batch의 cost 계산\n",
    "                avg_cost += cost / total_batch\n",
    "                avg_perplexity += cost / (total_batch * (sum(map(sum,batch))/batch_size))\n",
    "\n",
    "            total_costs[epoch] = avg_cost # epoch의 cost를 계산하여 저장\n",
    "            total_perplexity[epoch] = np.exp(avg_perplexity) # epoch의 perplexity를 계산하여 저장\n",
    "\n",
    "            print(\"Epoch : \",'%04d'%(epoch+1),\" Cost = \",\"{:.9f}\".format(avg_cost),\" training perplexity = \",\"{:.9f}\".format(np.exp(avg_perplexity)))\n",
    "            print(\"Elapsed Time : \" + str((time.time() - startTime)))\n",
    "\n",
    "        # epoch에 대한 cost 변화를 그래프로 표현\n",
    "        plt.plot(total_costs) \n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('cost')\n",
    "        plt.show()\n",
    "        return\n",
    "    \n",
    "    def test(self,testX,batch_size=1):\n",
    "        # 학습된 NVDM을 testX 데이터를 이용하여 testing하는 메소드\n",
    "        # Input : testX, batch_size\n",
    "        # Output : None\n",
    "\n",
    "        avg_perplexity = 0. # testing set의 average perplexity를 저장\n",
    "        total_batch = int(len(testX)/batch_size) # 전체 batch의 갯수\n",
    "\n",
    "        for i in range(total_batch): # total_batch만큼 반복 실행\n",
    "            batch = []\n",
    "            for j in range(batch_size): # testX에서 i번째 batch를 가져오는 과정\n",
    "                batch.append(testX[i*batch_size+j])\n",
    "            cost = self.sess.run(self.cost,feed_dict={self.x:batch})\n",
    "            avg_perplexity += cost / (total_batch * (sum(map(sum,batch))/batch_size))\n",
    "\n",
    "        print(\"testing perplexity = \",\"{:.9f}\".format(np.exp(avg_perplexity)))\n",
    "        return\n",
    "    \n",
    "    def topic_words(self, num_words=10):\n",
    "        # 학습된 NVDM의 R 행렬을 통하여 topic별 대표 단어를 뽑아내는 메소드\n",
    "        # Input : num_words\n",
    "        # Output : None\n",
    "        \n",
    "        R = self.sess.run(self.R)\n",
    "        top = [[] for _ in range(len(R))]\n",
    "        \n",
    "        print(\"Top \",'%2d'%(num_words),\" words in each topic\\n\")\n",
    "        \n",
    "        for i in range(len(R)):\n",
    "            index = (-R[i]).argsort()\n",
    "            top_idx = index[:num_words]\n",
    "            s = 'Topic %2d : '%(i + 1)\n",
    "            for word in bagVocab[top_idx]:\n",
    "                s += ' %s,' % (word)\n",
    "            print(s[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVDM with 50 dimension of latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRUeuuxRlftj",
    "outputId": "c91897c7-8f15-46df-a02a-3d71d4fd7708",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading start!\n",
      "Data loading finished!\n"
     ]
    }
   ],
   "source": [
    "print(\"Data loading start!\")\n",
    "\n",
    "bagVocab = np.load('bagVocab_3+_newstop_tfidf.npy') # 사용할 단어 목록을 정리한 파일 load (3글자 이상, stop words가 제거된 tf-idf 상위 2000개 단어)\n",
    "vect = CountVectorizer(vocabulary=bagVocab, binary=True)  # 단어를 count하는 방법 설정\n",
    "\n",
    "# training 데이터를 세팅하는 과정\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',shuffle=True, random_state=0) # '20newsgroups' train 데이터셋을 load\n",
    "data = vect.fit_transform(newsgroups_train.data)  # 각 문서별 단어 개수 세기 진행\n",
    "\n",
    "trainX = []\n",
    "for i in range(data.shape[0]): # data.shape[0] = 문서 개수 / sparse matrix 파일형식으로 설정된 data를 tuple로 변환하는 과정\n",
    "    tempdata = data[i].toarray()\n",
    "    temptuple = tuple(map(tuple,tempdata))[0]\n",
    "    trainX.append(temptuple)\n",
    "\n",
    "# testing 데이터를 세팅하는 과정\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=0) # '20newsgroups' test 데이터셋을 load\n",
    "data_test = vect.fit_transform(newsgroups_test.data) # 각 문서별 단어 개수 세기 진행\n",
    "\n",
    "testX = []\n",
    "for i in range(data_test.shape[0]): # data.shape[0] = 문서 개수 / sparse matrix 파일형식으로 설정된 data를 tuple로 변환하는 과정\n",
    "    tempdata = data_test[i].toarray()\n",
    "    temptuple = tuple(map(tuple,tempdata))[0]\n",
    "    testX.append(temptuple)\n",
    "\n",
    "print(\"Data loading finished!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0001  Cost =  413.465723358  training perplexity =  1589.166116341\n",
      "Elapsed Time : 32.135401248931885\n",
      "Epoch :  0002  Cost =  400.680911984  training perplexity =  1262.786851761\n",
      "Elapsed Time : 67.58098793029785\n",
      "Epoch :  0003  Cost =  397.345879209  training perplexity =  1189.855703660\n",
      "Elapsed Time : 107.31324648857117\n",
      "Epoch :  0004  Cost =  395.042668435  training perplexity =  1142.025310856\n",
      "Elapsed Time : 137.18315362930298\n",
      "Epoch :  0005  Cost =  393.393789916  training perplexity =  1108.856345489\n",
      "Elapsed Time : 172.721421957016\n",
      "Epoch :  0006  Cost =  391.849370636  training perplexity =  1078.633179098\n",
      "Elapsed Time : 192.7139813899994\n",
      "Epoch :  0007  Cost =  390.632992635  training perplexity =  1055.353319352\n",
      "Elapsed Time : 224.7656397819519\n",
      "Epoch :  0008  Cost =  389.627024423  training perplexity =  1036.683404083\n",
      "Elapsed Time : 251.59528613090515\n",
      "Epoch :  0009  Cost =  388.672182606  training perplexity =  1019.344533119\n",
      "Elapsed Time : 281.761027097702\n",
      "Epoch :  0010  Cost =  388.027314313  training perplexity =  1007.321329766\n",
      "Elapsed Time : 303.4586088657379\n",
      "Epoch :  0011  Cost =  387.168482958  training perplexity =  992.247142920\n",
      "Elapsed Time : 326.9287624359131\n",
      "Epoch :  0012  Cost =  386.369195449  training perplexity =  978.101093301\n",
      "Elapsed Time : 352.3961887359619\n",
      "Epoch :  0013  Cost =  385.852344074  training perplexity =  968.981124469\n",
      "Elapsed Time : 381.7953658103943\n",
      "Epoch :  0014  Cost =  385.063035003  training perplexity =  955.564696052\n",
      "Elapsed Time : 405.3528878688812\n",
      "Epoch :  0015  Cost =  384.503946490  training perplexity =  945.858274424\n",
      "Elapsed Time : 432.25819993019104\n",
      "Epoch :  0016  Cost =  383.921563613  training perplexity =  936.225072069\n",
      "Elapsed Time : 483.0861015319824\n",
      "Epoch :  0017  Cost =  383.302291195  training perplexity =  925.789751979\n",
      "Elapsed Time : 519.9230222702026\n",
      "Epoch :  0018  Cost =  382.780587796  training perplexity =  917.405218224\n",
      "Elapsed Time : 544.3730323314667\n",
      "Epoch :  0019  Cost =  382.207695615  training perplexity =  907.959416015\n",
      "Elapsed Time : 570.1292927265167\n",
      "Epoch :  0020  Cost =  381.589484291  training perplexity =  898.024644433\n",
      "Elapsed Time : 594.4243903160095\n",
      "Epoch :  0021  Cost =  381.191972581  training perplexity =  891.567646479\n",
      "Elapsed Time : 617.3752570152283\n",
      "Epoch :  0022  Cost =  380.662305984  training perplexity =  883.295322664\n",
      "Elapsed Time : 642.84468126297\n",
      "Epoch :  0023  Cost =  380.247998533  training perplexity =  876.601039491\n",
      "Elapsed Time : 669.4284698963165\n",
      "Epoch :  0024  Cost =  379.868843281  training perplexity =  870.640411391\n",
      "Elapsed Time : 695.4285907745361\n",
      "Epoch :  0025  Cost =  379.462512261  training perplexity =  864.424361080\n",
      "Elapsed Time : 718.8771724700928\n",
      "Epoch :  0026  Cost =  379.013852820  training perplexity =  857.480055125\n",
      "Elapsed Time : 742.1068804264069\n",
      "Epoch :  0027  Cost =  378.598940419  training perplexity =  851.158384473\n",
      "Elapsed Time : 773.7147934436798\n",
      "Epoch :  0028  Cost =  378.190257655  training perplexity =  844.915084048\n",
      "Elapsed Time : 798.1258232593536\n",
      "Epoch :  0029  Cost =  377.825501839  training perplexity =  839.496565598\n",
      "Elapsed Time : 821.9801745414734\n",
      "Epoch :  0030  Cost =  377.555493380  training perplexity =  835.520401622\n",
      "Elapsed Time : 845.236864566803\n",
      "Epoch :  0031  Cost =  377.178546467  training perplexity =  829.801516371\n",
      "Elapsed Time : 869.7658276557922\n",
      "Epoch :  0032  Cost =  376.929003960  training perplexity =  826.209504051\n",
      "Elapsed Time : 893.56320977211\n",
      "Epoch :  0033  Cost =  376.550924386  training perplexity =  820.473129932\n",
      "Elapsed Time : 916.3761548995972\n",
      "Epoch :  0034  Cost =  376.385882352  training perplexity =  818.122236788\n",
      "Elapsed Time : 939.2960407733917\n",
      "Epoch :  0035  Cost =  376.050755594  training perplexity =  813.190974311\n",
      "Elapsed Time : 963.0334575176239\n",
      "Epoch :  0036  Cost =  375.911354267  training perplexity =  811.121498727\n",
      "Elapsed Time : 986.2981443405151\n",
      "Epoch :  0037  Cost =  375.678712828  training perplexity =  807.899215538\n",
      "Elapsed Time : 1009.6827616691589\n",
      "Epoch :  0038  Cost =  375.343065380  training perplexity =  802.958642491\n",
      "Elapsed Time : 1032.4997055530548\n",
      "Epoch :  0039  Cost =  375.091470634  training perplexity =  799.383739163\n",
      "Elapsed Time : 1056.625900030136\n",
      "Epoch :  0040  Cost =  374.935577663  training perplexity =  797.082213099\n",
      "Elapsed Time : 1079.9675414562225\n",
      "Epoch :  0041  Cost =  374.628270512  training perplexity =  792.813468349\n",
      "Elapsed Time : 1102.4106991291046\n",
      "Epoch :  0042  Cost =  374.570394600  training perplexity =  791.973454787\n",
      "Elapsed Time : 1126.121132850647\n",
      "Epoch :  0043  Cost =  374.327553808  training perplexity =  788.550134028\n",
      "Elapsed Time : 1149.392816066742\n",
      "Epoch :  0044  Cost =  374.231238441  training perplexity =  787.148868258\n",
      "Elapsed Time : 1172.9383397102356\n",
      "Epoch :  0045  Cost =  373.992747349  training perplexity =  783.851956628\n",
      "Elapsed Time : 1196.3269572257996\n",
      "Epoch :  0046  Cost =  373.889242797  training perplexity =  782.382872135\n",
      "Elapsed Time : 1220.2472689151764\n",
      "Epoch :  0047  Cost =  373.786962999  training perplexity =  780.961266565\n",
      "Elapsed Time : 1245.226974248886\n",
      "Epoch :  0048  Cost =  373.593905829  training perplexity =  778.215766127\n",
      "Elapsed Time : 1267.798058271408\n",
      "Epoch :  0049  Cost =  373.474215887  training perplexity =  776.549544402\n",
      "Elapsed Time : 1291.5764501094818\n",
      "Epoch :  0050  Cost =  373.330863210  training perplexity =  774.557353905\n",
      "Elapsed Time : 1315.1739475727081\n",
      "Epoch :  0051  Cost =  373.217629222  training perplexity =  773.061734610\n",
      "Elapsed Time : 1338.0938313007355\n",
      "Epoch :  0052  Cost =  373.051337858  training perplexity =  770.685715779\n",
      "Elapsed Time : 1360.5909581184387\n",
      "Epoch :  0053  Cost =  372.975251594  training perplexity =  769.660708634\n",
      "Elapsed Time : 1384.0075595378876\n",
      "Epoch :  0054  Cost =  372.771185141  training perplexity =  766.839941336\n",
      "Elapsed Time : 1407.6230449676514\n",
      "Epoch :  0055  Cost =  372.639708933  training perplexity =  765.045684437\n",
      "Elapsed Time : 1429.7273952960968\n",
      "Epoch :  0056  Cost =  372.705247457  training perplexity =  765.837410009\n",
      "Elapsed Time : 1453.235942363739\n",
      "Epoch :  0057  Cost =  372.518123120  training perplexity =  763.369767750\n",
      "Elapsed Time : 1476.8234446048737\n",
      "Epoch :  0058  Cost =  372.412654640  training perplexity =  761.885251335\n",
      "Elapsed Time : 1500.213062286377\n",
      "Epoch :  0059  Cost =  372.326124343  training perplexity =  760.751128315\n",
      "Elapsed Time : 1521.6308043003082\n",
      "Epoch :  0060  Cost =  372.093609835  training perplexity =  757.603589594\n",
      "Elapsed Time : 1546.9693043231964\n",
      "Epoch :  0061  Cost =  372.064134446  training perplexity =  757.266270626\n",
      "Elapsed Time : 1571.1094906330109\n",
      "Epoch :  0062  Cost =  371.936958516  training perplexity =  755.497138529\n",
      "Elapsed Time : 1593.9963936805725\n",
      "Epoch :  0063  Cost =  371.805143103  training perplexity =  753.685389314\n",
      "Elapsed Time : 1616.5814719200134\n",
      "Epoch :  0064  Cost =  371.675481205  training perplexity =  751.967419414\n",
      "Elapsed Time : 1640.0320556163788\n",
      "Epoch :  0065  Cost =  371.660474659  training perplexity =  751.773765162\n",
      "Elapsed Time : 1663.6845161914825\n",
      "Epoch :  0066  Cost =  371.484266973  training perplexity =  749.356348276\n",
      "Elapsed Time : 1685.8558292388916\n",
      "Epoch :  0067  Cost =  371.457164933  training perplexity =  749.108964414\n",
      "Elapsed Time : 1708.529854297638\n",
      "Epoch :  0068  Cost =  371.440889612  training perplexity =  748.802640474\n",
      "Elapsed Time : 1733.0738098621368\n",
      "Epoch :  0069  Cost =  371.351261645  training perplexity =  747.615610171\n",
      "Elapsed Time : 1755.9297287464142\n",
      "Epoch :  0070  Cost =  371.227165290  training perplexity =  746.001520056\n",
      "Elapsed Time : 1778.9085788726807\n",
      "Epoch :  0071  Cost =  371.130893133  training perplexity =  744.646690484\n",
      "Elapsed Time : 1801.5706136226654\n",
      "Epoch :  0072  Cost =  371.123693686  training perplexity =  744.530629321\n",
      "Elapsed Time : 1825.8627116680145\n",
      "Epoch :  0073  Cost =  371.116773487  training perplexity =  744.459260144\n",
      "Elapsed Time : 1848.9914784431458\n",
      "Epoch :  0074  Cost =  371.084722198  training perplexity =  743.966658069\n",
      "Elapsed Time : 1872.4300634860992\n",
      "Epoch :  0075  Cost =  371.003427421  training perplexity =  742.947761765\n",
      "Elapsed Time : 1895.5708220005035\n",
      "Epoch :  0076  Cost =  370.963292755  training perplexity =  742.474477623\n",
      "Elapsed Time : 1920.4425888061523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0077  Cost =  370.832965952  training perplexity =  740.644817369\n",
      "Elapsed Time : 1955.5225138664246\n",
      "Epoch :  0078  Cost =  370.814502817  training perplexity =  740.497210744\n",
      "Elapsed Time : 1987.963355064392\n",
      "Epoch :  0079  Cost =  370.789960743  training perplexity =  740.197019139\n",
      "Elapsed Time : 2013.107965707779\n",
      "Epoch :  0080  Cost =  370.789299349  training perplexity =  740.139887598\n",
      "Elapsed Time : 2036.4815905094147\n",
      "Epoch :  0081  Cost =  370.610413948  training perplexity =  737.667488381\n",
      "Elapsed Time : 2060.3429362773895\n",
      "Epoch :  0082  Cost =  370.685553087  training perplexity =  738.715945821\n",
      "Elapsed Time : 2081.513822078705\n",
      "Epoch :  0083  Cost =  370.516836791  training perplexity =  736.431796283\n",
      "Elapsed Time : 2104.4916734695435\n",
      "Epoch :  0084  Cost =  370.487828348  training perplexity =  736.232547861\n",
      "Elapsed Time : 2126.2921979427338\n",
      "Epoch :  0085  Cost =  370.361386459  training perplexity =  734.517829855\n",
      "Elapsed Time : 2149.0171933174133\n",
      "Epoch :  0086  Cost =  370.314304318  training perplexity =  733.820874611\n",
      "Elapsed Time : 2171.1375353336334\n",
      "Epoch :  0087  Cost =  370.276555424  training perplexity =  733.442259710\n",
      "Elapsed Time : 2192.546287059784\n",
      "Epoch :  0088  Cost =  370.168592335  training perplexity =  731.950724091\n",
      "Elapsed Time : 2214.815541267395\n",
      "Epoch :  0089  Cost =  370.111363774  training perplexity =  731.224779987\n",
      "Elapsed Time : 2237.912322998047\n",
      "Epoch :  0090  Cost =  370.070966602  training perplexity =  730.789236337\n",
      "Elapsed Time : 2262.4772667884827\n",
      "Epoch :  0091  Cost =  370.116655468  training perplexity =  731.289524420\n",
      "Elapsed Time : 2283.472252845764\n",
      "Epoch :  0092  Cost =  370.055566568  training perplexity =  730.488855664\n",
      "Elapsed Time : 2306.680972337723\n",
      "Epoch :  0093  Cost =  370.097728628  training perplexity =  731.059246686\n",
      "Elapsed Time : 2329.7507712841034\n",
      "Epoch :  0094  Cost =  370.059130643  training perplexity =  730.451689780\n",
      "Elapsed Time : 2352.0969829559326\n",
      "Epoch :  0095  Cost =  370.075245815  training perplexity =  730.653897655\n",
      "Elapsed Time : 2373.067982673645\n",
      "Epoch :  0096  Cost =  369.936458081  training perplexity =  728.899634445\n",
      "Elapsed Time : 2396.4356122016907\n",
      "Epoch :  0097  Cost =  369.962113642  training perplexity =  729.286772917\n",
      "Elapsed Time : 2418.773827314377\n",
      "Epoch :  0098  Cost =  369.962691316  training perplexity =  729.289058027\n",
      "Elapsed Time : 2440.8741817474365\n",
      "Epoch :  0099  Cost =  369.881540214  training perplexity =  728.240314694\n",
      "Elapsed Time : 2467.026215314865\n",
      "Epoch :  0100  Cost =  369.830905881  training perplexity =  727.565920482\n",
      "Elapsed Time : 2497.3738489151\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "# input(2000) -> h1(500) -> h2(500) -> z(50) -> output(2000)\n",
    "network_architecture = dict(n_hidden_recog_1=500, n_hidden_recog_2=500, n_input=data.shape[1], n_z=50)\n",
    "\n",
    "nvdm = NVDM(network_architecture)\n",
    "nvdm.train(trainX,batch_size=100,training_epochs=100,learning_rate=0.001) # NVDM training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과해석부1\n",
    "\n",
    "위의 출력 결과는 20NewsGroups 데이터셋을 이용해 50차원 document representation NVDM으로 학습한 결과입니다.\n",
    "\n",
    "20NewsGroups 데이터셋은 약 18,000개의 newsgroups posts을 20개의 토픽으로 분류해 놓은 데이터셋으로, 특정 날짜를 기준으로 training 게시글과 testing 게시글로 나뉘어져 있습니다.\n",
    "위 모델에서는 모든 게시글의 단어 중 stop words를 제외하고 3글자 이상의 단어의 TF-IDF 값 상위 2,000개의 단어만을 사용했습니다.\n",
    "위 모델의 Encoder는 2개의 500차원 Hidden Layer(activation function : ReLU)를 가진 MLP이며 batch size는 100, epoch size는 100, learning rate는 0.001입니다.\n",
    "\n",
    "출력문은 각 epoch의 average cost와 traning perplexity, 실행 소요 시간을 나타냈습니다. 출력문 아래의 그래프는 x축은 epoch, y축은 cost로 epoch이 진행되면서 cost의 변화를 그래프로 표현한 것입니다. \n",
    "\n",
    "perplexity란 topic modeling에서 성능을 평가하기 위해 사용하는 지표인데, $exp(-\\frac{1}{D} \\sum_{n}^N \\frac{1}{N_{d}}logp(X_{d}))$(단 D는 document의 개수, $N_{d}$는 dth document의 단어의 개수) 로 계산합니다. 단, NVDM에서는 logp(X)를 구하기 힘드므로, variational lower bound로 대체하면 perplexity의 upper bound를 계산할 수 있습니다.\n",
    "\n",
    "\n",
    "그래프에서 알 수 있듯이, 학습이 진행되면서(epoch이 증가하면서) cost/perplexity가 꾸준히 감소함을 알 수 있습니다. \n",
    "이를 통해서 cost 식이 감소하는 방향으로 학습이 잘 진행되고 있음을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hxkrgHCclftp",
    "outputId": "a6ba6655-570b-4890-ae47-ea00cd1419e4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "testing perplexity =  726.911605288\n",
      "==============================================\n",
      "Top  10  words in each topic\n",
      "\n",
      "Topic  1 :  rob, rice, saturn, bios, thor, split, wait, convex, caused, answers\n",
      "Topic  2 :  bradley, csd, midway, utk, win, gatech, ati, hurt, chicago, manager\n",
      "Topic  3 :  princeton, rit, sports, saturn, sas, philadelphia, isc, society, turbo, thor\n",
      "Topic  4 :  upenn, psuvm, cunixb, psu, sas, seas, ide, master, genesis, arabs\n",
      "Topic  5 :  forward, bunch, replies, players, excuse, stats, digex, compare, greg, maynard\n",
      "Topic  6 :  experiences, ncsu, anderson, heavy, eff, minnesota, eos, staff, greatly, employer\n",
      "Topic  7 :  adapter, buffalo, mary, stratus, chi, slot, devils, dare, ide, bus\n",
      "Topic  8 :  lord, christ, church, christians, father, catholic, jewish, scripture, greek, athos\n",
      "Topic  9 :  patients, medicine, boeing, vnews, activities, udel, vax, communication, kent, sandvik\n",
      "Topic 10 :  jobs, citizens, budget, thor, congress, americans, clinton, tax, social, health\n",
      "Topic 11 :  francisco, rider, frank, jose, guest, motorcycle, uchicago, dean, alomar, san\n",
      "Topic 12 :  bank, ottawa, mot, ericsson, stein, diego, islam, building, muslims, ecs\n",
      "Topic 13 :  ottawa, popular, hewlett, packard, linux, buffalo, taxes, america, radio, hear\n",
      "Topic 14 :  craig, holland, bell, budget, harvard, colostate, toronto, virginia, wilson, domain\n",
      "Topic 15 :  yale, vram, simms, motorola, berkeley, msg, hey, mot, rom, simm\n",
      "Topic 16 :  pages, clock, canon, circuit, views, jake, politics, bus, brain, mode\n",
      "Topic 17 :  detector, arc, electrical, rpi, radar, mary, ecs, sfu, portal, loss\n",
      "Topic 18 :  northern, ottawa, bnr, mcgill, ontario, vancouver, bell, virtual, thor, stealth\n",
      "Topic 19 :  ysu, yfn, duke, motorcycle, insurance, jack, ksu, cray, oracle, cellular\n",
      "Topic 20 :  centre, predictions, nick, tel, tue, ontario, sas, sci, shuttle, postings\n",
      "Topic 21 :  sfu, fraser, nec, scsi, behanna, vga, norton, vesa, rom, motherboard\n",
      "Topic 22 :  taxes, wright, stealth, cmu, monitors, diamond, shuttle, billion, ece, capital\n",
      "Topic 23 :  fonts, font, virtual, vga, bios, svga, polygon, directory, select, postscript\n",
      "Topic 24 :  advance, ftp, windows, thanks, email, dos, info, appreciated, comp, graphics\n",
      "Topic 25 :  jose, san, ken, diego, iowa, ames, bay, optilink, california, clayton\n",
      "Topic 26 :  yale, engin, mailing, poster, midway, ann, arbor, umn, msstate, vnet\n",
      "Topic 27 :  clock, iowa, spencer, damage, cache, dseg, ins, boulder, western, intel\n",
      "Topic 28 :  voltage, amp, ecn, circuit, uxa, caps, logic, christopher, adams, zoo\n",
      "Topic 29 :  georgia, athena, uga, notes, higgins, athens, gatech, covington, prism, watson\n",
      "Topic 30 :  tank, morning, miles, lights, battery, minutes, alive, went, fix, button\n",
      "Topic 31 :  graham, cray, wondering, gtoal, wheel, douglas, section, johnson, translation, listen\n",
      "Topic 32 :  cat, blame, trial, effect, edge, mantis, honda, tony, gas, replies\n",
      "Topic 33 :  gateway, speech, voice, noticed, middle, germany, drives, bios, boot, connection\n",
      "Topic 34 :  users, companies, vice, traffic, communication, labs, shall, tue, issues, zip\n",
      "Topic 35 :  monitors, environment, nec, groups, earth, wide, umich, concept, adapter, chris\n",
      "Topic 36 :  academic, detroit, oakland, ted, cubs, uic, louis, uicvm, medicine, defense\n",
      "Topic 37 :  sexual, sick, options, drugs, packard, rich, jsc, hewlett, adam, tim\n",
      "Topic 38 :  valid, backup, academic, danny, band, lost, automatic, mantis, sox, morris\n",
      "Topic 39 :  digex, express, criminals, amendment, lance, online, pat, communications, access, atf\n",
      "Topic 40 :  randy, oracle, serial, determine, laurentian, allows, court, report, event, false\n",
      "Topic 41 :  lewis, excellent, gone, necessarily, field, poor, boot, sam, weight, machines\n",
      "Topic 42 :  expressed, motorola, seattle, significant, handling, davis, environment, treatment, energy, optilink\n",
      "Topic 43 :  utexas, reference, gmt, written, navy, islam, german, reader, author, postings\n",
      "Topic 44 :  pub, eos, psuvm, ftp, zip, psu, sites, anonymous, ncsu, msu\n",
      "Topic 45 :  uga, athena, georgia, mcovingt, athens, johnson, covington, hudson, resurrection, seas\n",
      "Topic 46 :  jsc, fnal, arc, higgins, stanford, packard, hewlett, leland, bear, nasa\n",
      "Topic 47 :  mailing, round, changed, request, nmsu, wide, colorado, apply, surface, continue\n",
      "Topic 48 :  max, carson, adobe, col, sdsu, seattle, tech, cray, tom, postscript\n",
      "Topic 49 :  umn, player, col, staff, fpu, slot, pitching, expansion, mhz, carson\n",
      "Topic 50 :  koresh, ksu, roby, satan, okcforum, mcovingt, magnus, atheists, covington, atheist\n"
     ]
    }
   ],
   "source": [
    "print(\"==============================================\")\n",
    "nvdm.test(testX) # NVDM testing\n",
    "print(\"==============================================\")\n",
    "nvdm.topic_words()  # topic별로 단어를 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GRkEd7ETlftt"
   },
   "source": [
    "### 결과해석부2\n",
    "\n",
    "출력문을 통해 testing perplexity가 726.91로 나타남을 알 수 있습니다. testing set을 이용해 학습된 모델의 perplexity를 계산한 것으로, 다른 document model과 성능 비교를 할 때, 이 수치를 이용할 수 있습니다.\n",
    "\n",
    "latent variable의 각 dimension이 각각 하나의 Topic을 뜻한다고 가정하겠습니다. 따라서 Topic은 50개가 됩니다. 그 이후 모델을 통해 학습된 각 주제의 상위 10개 단어를 결과로 나타내겠습니다. 이는 각 dimension이 어떤 단어에 큰 확률을 가지는 지를 뜻합니다. 즉, R matrix의 각 row가 Topic이 되고 column이 단어가 되어 row별로 값이 큰 단어를 차례대로 나타낸 것입니다.\n",
    "예를 들어, topic 8에서 주로 christian에 관한 단어들이 대표 단어로 추출되었습니다. 이를 통해 topic 8이 20NewsGroups 데이터셋의 토픽 중 'soc.religion.christian'에 관한 기사들을 잘 모은 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Chapter_17-NVDM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
